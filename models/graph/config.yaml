# configuration for deepwalk
# -----------paddlcloud 配置: 主要是一些提交paddlecloud任务的信息----------------------#
job_name: test_nlp_pglbox_deepwalk_singlenode
# paddlecloud GPU 队列名称，每个部门的队列不同
group_name: 
# paddlecloud 提交任务所使用的docker 镜像
image_addr: 
k8s_priority: high
wall_time: "240000:00:00"
# 要用来训练的GPU卡
gpus: [0, 1, 2, 3, 4, 5, 6, 7]

# ---------------------------数据配置-------------------------------------------------#
# 图数据所在hadoop集群的fs_name和ugi
graph_data_fs_name: ""
graph_data_fs_ugi: ""
# graph_data_hdfs_path: 图数据在hadoop集群存放的位置。
graph_data_hdfs_path: ""
#graph_data_local_path: "/raid0/gdata/"
graph_data_local_path: "./data/"

# 每种边类型对应的文件或者目录, 这里u2u_edges.txt 对应graph_data_hdfs_path目录下的u2u_edges.txt文件, 其它的以此类推。
# 如果u和i之间既有点击关系，有可能有关注关系，那么可以用三元组表示，即用：u2click2i 和 u2focus2i 来表示点击和关注这两种不同的关系
etype2files: "cuid2clk:cuid2clk,cuid2conv:cuid2conv,cuid2fcclk:cuid2fcclk,cuid2fcconv:cuid2fcconv,clk2entity:clk2entity,conv2entity:conv2entity"
# 每种节点类型对应的文件或目录, 不同类型的节点可以放在同个文件，读取的时候会自动过滤的。
ntype2files: "cuid:node_types,clk:node_types,conv:node_types,fcclk:node_types,fcconv:node_types,entity:node_types"
# the pair specified by 'excluded_train_pair' will not be trained, eg "w2q;q2t" 
excluded_train_pair: ""
# only infer the node(s) specified by 'infer_node_type', eg "q;t"
infer_node_type: ""
# label of train pair, eg "cuid2conv:2,cuid2fcclk:3,cuid2fcconv:4,clk2entity:5,conv2entity:6"
pair_label: ""

# 如果要使用hadoop_shard 的功能, 请先参考./tools/graph_sharding/README.md 进行操作; 这个参数的功能是手动对数据进行分片, 这样可以减少每台机从hadoop拉取数据的数据量，加快拉取速度。
# 目前使用graph_shard 的功能, 对数据进行分片，必须分片为1000 个part文件
train_start_nodes: ""
infer_nodes: ""
hadoop_shard: True
# auto_shard是对数据自动进行sharding，强烈建议开启auto_shard，这样用户就不要先手动对数据进行sharding了。但如果用户已经使用hadoop streaming对数据手动sharding了，那就不要开启auto_shard了。
auto_shard: False
# num_part: 1000
num_part: 10
# 是否双向边（目前只支持双向边）
symmetry: True

# ---------------------------图游走配置-------------------------------------------------#
# meta_path 元路径定义。 注意跟etype2files变量的边类型对应。用“-”隔开
# 按照下面的设置可以设置多条metapath, 每个";"号是一条用户定义的metapath
meta_path: "cuid2clk-clk2cuid;cuid2conv-conv2cuid;cuid2fcclk-fcclk2cuid;cuid2fcconv-fcconv2cuid;clk2cuid-cuid2clk;conv2cuid-cuid2clk;fcclk2cuid-cuid2clk;fcconv2cuid-cuid2clk;clk2cuid-cuid2conv;conv2cuid-cuid2conv;fcclk2cuid-cuid2conv;fcconv2cuid-cuid2conv;clk2entity-entity2clk;conv2entity-entity2conv;clk2entity-entity2conv;conv2entity-entity2clk"

# 游走路径的正样本窗口大小
win_size: 3
# neg_num: 每对正样本对应的负样本数量
neg_num: 5
# walk_len: metapath 游走路径深度
walk_len: 24
# walk_times: 每个起始节点重复n次游走，这样可以尽可能把一个节点的所有邻居游走一遍，使得训练更加均匀。
walk_times: 10


# -------------------------slot特征的配置-----------------------------------------------#
# 节点ID 的embedding 表的slot, 保持默认即可
nodeid_slot: 9008
# 节点slot 特征配置，如果没有节点特征, 则slots参数为空列表
slots: [] #["1", "2", "3", "4", "5", "6", "7", "8"]
float_slots: []
# 格式是{"float_slot_id":shape...}，用法相当于token slot的max_seq_len,这里支持多个连续值特征，分别指定不同维度，那么python组网需要为每个连续值特征创建holder
# 如果在节点文件里将多个连续值向量concat到一起，那么python端组网值需要定义一个holder,并使用类似于split_token_section的方式来切分特征数据
float_slots_len: {}
# slot_pool_type: slot 特征的聚合方式，有concat或者sum， 一般不用改动, sum的效果就比较ok了。
slot_pool_type: sum
# max_slot_value_size: 一个slot如果对应的值太多，会很影响训练速度，甚至无法训练, 而且效果也不一定会更好。所以这里做了限制。一般不用改动。
max_slot_value_size: 100
# feature_mode: concat or sum, slot特征之间的交互方式，一般不用改动，sum的效果就比较好了。
feature_mode: sum

# ------------------模型与向量的输出目录配置---------------------------------------#
# 训练结果输出的hadoop目录
# 训练结果所要保存的hadoop集群的fs_name和ugi, 可以和图数据保存在不同的hadoop集群
fs_name: ""
fs_ugi: ""
# working_root: 训练结果(模型和embedding)的输出目录，支持paddlecloud本地目录和hadoop目录。
# 如果需要输出到hadoop目录，则working_root需要有afs或hdfs前缀，例如：afs:/your/hadoop/path
#working_root: /raid0/working_root
working_root: ./working_root
# pdc_info_output_path: paddlecloud 需要一个hadoop目录保存提交任务的相关代码信息。这里随便填一个hadoop路径即可。
pdc_info_output_path: 

# warm_start_from: 训练阶段需要热启的模型所在的hadoop路径
warm_start_from: null
# 热启时加载二进制模型，仅在SSD模式下生效
load_binary_mode : False

# ---------------------------模型参数配置---------------------------------------------#
# 模型类型选择
model_type: GNNModel
# embedding 维度
emb_size: 64
# sparse_type: 稀疏参数服务器的优化器，目前支持adagrad, shared_adam
sparse_type: adagrad_v2
# sparse_lr: 稀疏参数服务器的学习率
sparse_lr: 0.05
# dense_lr: 稠密参数的学习率
dense_lr: 0.000005 #001
# slot_feature_lr: slot特征的学习率
slot_feature_lr: 0.001
init_range: 0.1
# loss_type: 损失函数，目前支持hinge; sigmoid; nce
loss_type: nce
margin: 2.0  # for hinge loss
# 如果slot 特征很多(超过5个), 建议开启softsign，防止数值太大。
softsign: False
# 对比学习损失函数选择： 目前支持 simgcl_loss
gcl_loss: simgcl_loss

# sage 模型设置开关
sage_mode: False
# sage_layer_type: sage 模型类型选择
sage_layer_type: "LightGCN"
sage_alpha: 0.9
samples: [5]
infer_samples: [100]
sage_act: "relu"

# 带权采样开关，同时作用于游走和sage子图采样阶段。
weighted_sample: False
# 是否返回边权重，仅用于sage_mode=True模式，如边不带权则默认返回1。
return_weight: False
# 是否要进行训练，如果只想单独热启模型做预估(inference)，则可以关闭need_train
need_train: True
# 是否需要进行inference. 如果只想单独训练模型，则可以关闭need_inference
need_inference: True
# 预估embedding的时候，需要的参数，保持默认即可.
dump_node_name: "src_node___id"
dump_node_emb_name: "src_node___emb"
# 是否需要dump游走路径
need_dump_walk: False

# ---------------------------train param config---------------------------------------------#
epochs: 1
# 训练样本的batch_size
batch_size: 80000
infer_batch_size: 80000
chunk_num: 1
# 1 for debug
debug_mode: 0
# 是否开启metapath优化
metapath_split_opt: False
# 训练模式，可填WHOLE_HBM/MEM_EMBEDDING/SSD_EMBEDDING，默认为MEM_EMBEDDING
train_storage_mode: MEM_EMBEDDING
# 大致设置gpups的显存占用率 默认 0.25
gpups_memory_allocated_rate: 0.60
version: v_refactor_0104

# ---------------------------例行训练和预测------------------------------#
# 若修改为"online_train"，则表示需要进行例行训练，否则默认为普通训练模式。
train_mode: "train"
# 例行训练的第一次图数据起始时间。（注意，天级别更新的时候，日期的格式也一定要是20230217/00, 不能省略小时的单位）
start_time: 20230217/00
# 例行训练的小时级间隔(当train_mode=online_train时生效)，time_delta=1表示每次训练一个小时数据，然后换到下一个小时。如果是天级别更新，则time_delta=24。
time_delta: 24

# ---------------------------save config---------------------------------------------#
# 为了加快训练速度，可以设置每隔多少个epochs才保存一次模型。默认最后一个epoch训练完一定会保存模型。
save_model_interval: 10
# 触发ssd cache的频率
save_cache_frequency: 4
# 内存中缓存多少个pass
mem_cache_passid_num: 4
# 是否保存为二进制模式，仅在SSD模式下生效
save_binary_mode : False
# 保存模型前，是否需要根据threshold删除embedding
need_shrink: False

#------------------------ eval config ---------------------------------#
# 是否开启运行完自动评估召回，目前只支持20220610数据验证
eval_recall: 1
